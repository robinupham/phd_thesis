
% % Uncomment to build this alone without subfiles:
% % (also stuff at bottom)
% \documentclass{scrbook}
% \input{../preamble}
% \pagestyle{headings}
% \setcounter{chapter}{7} % deliberately 1 too low
% \begin{document}

% Uncomment to use subfiles:
% \documentclass[../Thesis.tex]{subfiles}
% \begin{document}

\chapter{Conclusions}
\label{chap:conclusions}

\section{Summary}

Cosmology is in a peculiar state: on one hand, we have a model (\lcdm{}, described in \autoref{chap:cosmo}) that fits most observational data, particularly from the cosmic microwave background (CMB), with remarkable accuracy. On the other hand, almost nothing is understood about the two apparently dominant components of the Universe today---dark energy and dark matter, and it is not clear how to reconcile these components with the theories of gravity and of the standard model of particle physics.

Analysis of weak gravitational lensing by the large scale structure of the Universe has particular promise as an observational probe with which to make progress on these questions. The subtle distortions of the shapes of distant galaxies induced by the gravitational effect of large scale structure depends closely on how exactly this structure has formed and evolved as the Universe has expanded, which in turn depends on the details of dark energy, dark matter, and gravity. Precise measurements and statistical analysis of these distortions can therefore place tight constraints on the physical nature of the dominant components in the Universe.

This promise is set to be realised by an upcoming generation of weak lensing surveys, such as the \Euclid{} space mission, the Rubin optical observatory, and the Square Kilometre Array radio observatory. (These surveys and others are described in \autoref{chap:cosmo}.) This next generation will survey billions of galaxies and, for this reason, will reach unprecedented statistical precision in their cosmological constraints. This unprecedented precision will, in turn, demand unprecedented accuracy and understanding of every aspect of the analysis in order to obtain reliable results. The work presented in this thesis has helped to make progress towards this goal.

Chapters \ref{chap:exact_like}--\ref{chap:binning} have contributed to understanding the statistical properties of \pcl{} estimators, which are fast two-point correlation estimators in Fourier space designed for partial-sky observations and are described in \autoref{chap:est_like}. These will be used for \Euclid{} and have been used in a number of weak lensing analyses to date (see \autoref{chap:est_like} and references therein), and yet prior to the work presented in this thesis little was known about their statistical properties. In \autoref{chap:exact_like}, the exact joint likelihood function for an arbitrary number of \pcl{} estimates from an arbitrary number of correlated spin-0 and spin-2 Gaussian fields was derived. While the likelihood function is not practical to evaluate in full for a real cosmological analysis, and the Gaussian field assumption does not hold for weak lensing on all scales, this work was an essential stepping stone for the work presented in \autoref{chap:gauss_like}. This later work demonstrated, with a high degree of robustness to changes in the analysis setup or details of the underlying fields, that a Gaussian likelihood is a sufficiently accurate choice for placing constraints on dark energy with data from a \Euclid{}-like survey. This means that when results obtained using a Gaussian likelihood are surprising or suspicious, we can be confident that this is not the result of an inappropriate likelihood function. A Gaussian likelihood requires a covariance matrix, and in \autoref{chap:cov} a method was developed for calculating the covariance matrix of \pcl{} estimates, accounting for both the coupling of scales due to the effect of the mask, which describes the incomplete sky coverage, and the non-Gaussian coupling arising from non-linear structure growth on small physical scales. This covariance matrix was compared to an estimated covariance matrix measured from weak lensing simulations, with good agreement, and it was demonstrated that neglecting the non-Gaussian contributions leads to poor accuracy in parameter constraints. Finally, \autoref{chap:binning} considered the question of how to choose the number of angular bins used in a \pcl{} analysis, which is a topic that had not previously been investigated. This question was also investigated for real-space correlation functions, and it was revealed that while the statistical precision of both estimators converges for a similar number of angular bins, the divergence of this precision for a too-broad binning is dramatically different between the two. This is an important demonstration that results derived for one estimator cannot be automatically extended to others.

\autoref{chap:cnn} turned to another problem that must be addressed before reliable cosmological inference is possible with the next generation of weak lensing surveys, this time focusing on radio observations, for which shear estimation is a particular challenge. In \autoref{chap:cnn}, a machine learning approach to this challenge was investigated, using a simplified case of CMB lensing as a starting point. The results therein suggest that convolutional neural networks are a method of promise in this area.

\section{Future prospects}

Naturally many questions remain. For instance, while \autoref{chap:cov} provided a method for calculating the covariance matrix of \pcl{} estimates, and presented a comparison of the result with simulations, it did not rigorously quantify the accuracy of the method at the level required for \Euclid{}, nor did it compare different options at each step. How are the non-Gaussian contributions to the covariance best calculated? The super-sample covariance component dominates over the connected non-Gaussian component, and yet the latter cannot be neglected. The method used to calculate the connected non-Gaussian component in \autoref{chap:cov} is extremely slow, to the point of not being feasible for use when analysing future data from \Euclid{}. An approximation was presented, but is this sufficiently accurate? If not, can a better approximation be developed? The super-sample covariance component in \autoref{chap:cov} was calculated using the halo model. Is this approach sufficiently accurate? Should the response approach be used instead? Should the three-dimensional details of the survey volume be taken into account, describing not only the survey mask but the fact that the depth of the survey varies over its area? What about correlations between the signal and mask, which are likely to exist when dense regions are preferentially masked out? Meanwhile, the generally dominant component of the \pcl{} covariance is the Gaussian component. In \autoref{chap:cov}, this was calculated using the improved narrow kernel approximation (NKA), which was found to be significantly more accurate than the standard NKA, but is it sufficiently accurate for \Euclid{}? Perhaps it is necessary to move to an approximation closer to the known exact form. And how should we properly account for the combination of cut-sky mode coupling and non-Gaussian mode coupling? This was done in an approximate manner in \autoref{chap:cov}, but again it is unknown whether this is sufficiently accurate. Maybe a superior method can be derived that is still computationally feasible. Ideally answering these questions could be achieved with a large suite of fully realistic simulations, but that seems impossible when even a single realisation of the \Euclid{} flagship simulations takes close to a million node hours to evaluate.

Meanwhile, \autoref{chap:binning} considered the question of how to choose the number of angular bins used for two-point estimators with \Euclid{}. The results offer some useful data points but do not establish an ultimate answer to this question. On the contrary, it was found that the answer changes depending on which cosmological parameters are being constrained, how many parameters are being constrained, and other aspects such as scale cuts and noise levels. If the answer is truly so sensitive to all of these considerations then it becomes extremely complicated to be confident that the correct choice has been made for a particular analysis setup. Perhaps a universal solution can be found. On the other hand, perhaps it is best not to bin at all, at least for the power spectrum. This may be possible, but may cause problems due to the resulting length of the data vector. For the correlation function, though, this is not an option, since the data are not fundamentally discrete. Another interesting result in \autoref{chap:binning} is that the stark difference in constraining power between the power spectrum and correlation function with an extremely coarse binning can be explained by the different ways in which scales are weighed within bins between the two statistics. This raises a further question of how this weighting should be chosen, if it has the ability to make such a difference to the results. Finally, there is the accompanying question of how best to bin in redshift space, and how this may itself interact with the choice of angular binning strategy.

A further aspect of the use of any estimator that must be considered for its use for precision cosmology is the modelling of the signal, including the contribution from noise. A forward-modelling approach is being taken for the \pcl{} estimator for \Euclid{}, which means that essentially the data are left alone, and the theory prediction to the data is instead treated to contain all the contributions that have entered the observations. Modelling the signal correctly is a big challenge: there are significant challenges on the theoretical side, but there are also many estimator effects that must be accurately modelled. How can we be confident that these cumulative effects are modelled sufficiently accurately? Comparisons between results obtained with different estimators are valuable, but it is difficult to be certain that no systematic errors remain. Different estimators naturally produce different results because of their distinct natures, such as how they weight different scales, meaning that there is no perfect agreement even in the absence of any systematic errors. This is a challenge that will require more work in order to have full confidence in the results obtained using any estimators for \Euclid{}.

Moving on from considering \Euclid{} and \pcl{} estimates, \autoref{chap:cnn} considered the use of convolutional neural networks (CNNs) for weak lensing estimation. The results show that CNNs for this purpose have promise, but the method developed in that chapter is far from a finished method for the ultimate aim of estimating shear from radio visibilities. The chapter focuses instead on the case of CMB lensing, but much work needs to be done in order to develop this into a method that is viable for current or future CMB experiments. Many more challenges lie ahead before the method would be suitable for radio observations with the Square Kilometre Array (SKA), which are discussed in the chapter. As with the previous chapters, there still exists a fundamental challenge with testing any candidate method, because this ideally requires full-realism simulations of SKA observations, with a sufficient number of realisations to detect subtle systematic errors, which is extremely challenging computationally.

Besides all of the questions laid out above, which follow on immediately from the work presented in this thesis, there are many more questions that must be answered in advance of receiving data from the Stage IV generation of weak lensing experiments such as \Euclid{} and the SKA. Every aspect of the analysis must be intricately understood: how to treat all the data appropriately, and how to model everything theoretically. The ultimate aim is to reach a point where we can fully believe whatever the results from this upcoming generation of experiments may tell us about the nature of the Universe.


% % Uncomment to build alone without subfiles:
% % \printbibliography[heading=bibintoc]
% \end{document}
